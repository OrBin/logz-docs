---
layout: article
title: Set up your Service Performance Monitoring dashboard
permalink: /user-guide/distributed-tracing/service-performance-monitoring-setup
image: https://dytvr9ot2sszz.cloudfront.net/logz-docs/social-assets/docs-social.jpg
description: Setting up your Service Performance Monitoring dashboard
flags:
  beta: true
  logzio-plan: pro enterprise
tags: 
  - distributed tracing
contributors:
  - hidan
---

Logz.io Service Performance Monitoring dashboard provides an overview of your systems' health by aggregating **Request**, **Error** and **Duration** (R.E.D) metrics from span data. The dashboard helps you to pinpoint and isolate incidents in your system quickly.

{% include page-info/early-access.md type="Beta" %}


The following guide helps you configure the OpenTelemetry collector to receive metrics from spans generated by your OpenTelemetry installation and send them to Logz.io. On deployment, the collector receives the span metrics and breaks them into 2 tracks:

* One track sends the spans back to Logz.io's backend as is
* The second track extracts the metrics out of the data, and sends it to Logz.io's Prometheus based account

To do so, Logz.io creates a **Metrics** account (based on Prometheus), generating an additional token needed to instrument the process. To connect your Tracing and Metrics account, you need to make sure the latter contains the name of your existing Tracing account. 

For example, if your Tracing account is called `Sock Shop Tracing`, the connected Metrics account can be `SPM Sock Shop Tracing`. 

![Tracing and Metrics account overview](https://dytvr9ot2sszz.cloudfront.net/logz-docs/distributed-tracing/tracing-and-metrics-accounts.png)


<div class="tasklist">

#### Set up your locally hosted OpenTelemetry installation to send traces to Logz.io

Before you begin, you’ll need:

* An application instrumented with an OpenTelemetry installation
* An active Tracing account with Logz.io

##### Add Logz.io exporter to your OpenTelemetry collector
Add the following parameters to the configuration file of your OpenTelemetry collector:

* Under the `receivers` list:

```yaml
otlp/spanmetrics:
    protocols:
      grpc:
        endpoint: :12345
  prometheus:
    config:
      global:
        external_labels:
          p8s_logzio_name: spm-demo-otel
      scrape_configs: 
      - job_name: 'atm'
        scrape_interval: 15s
        static_configs:
        - targets: [ "0.0.0.0:8889" ]
```

* Under the `exporters` list:

```yaml
prometheusremotewrite:
    endpoint: https://<<LISTENER-HOST>>:8053
    headers:
      Authorization: Bearer <<PROMETHEUS-METRICS-SHIPPING-TOKEN>>
  prometheus:
    endpoint: "localhost:8889"
```

* Under the `processors` list:

```yaml
spanmetrics:
    metrics_exporter: prometheus
    latency_histogram_buckets: [2ms, 6ms, 10ms, 100ms, 250ms, 500ms, 1000ms, 10000ms, 100000ms, 1000000ms]
    # Additional list of dimensions on top of:
    # - service.name
    # - operation
    # - span.kind
    # - status.code
    dimensions:
      # If the span is missing http.method, the processor will insert
      # the http.method dimension with value 'GET'.
      # For example, in the following scenario, http.method is not present in a span and so will be added as a dimension to the metric with value "GET":
      # - promexample_calls{http_method="GET",operation="/Address",service_name="shippingservice", span_kind="SPAN_KIND_SERVER",status_code="STATUS_CODE_UNSET"} 1
      - name: http.method
        default: GET
      # If a default is not provided, the http.status_code dimension will be omitted
      # if the span does not contain http.status_code.
      # For example, consider a scenario with two spans, one span having http.status_code=200 and another missing http.status_code. Two metrics would result with this configuration, one with the http_status_code omitted and the other included:
      # - promexample_calls{http_status_code="200",operation="/Address",service_name="shippingservice", span_kind="SPAN_KIND_SERVER",status_code="STATUS_CODE_UNSET"} 1
      # - promexample_calls{operation="/Address",service_name="shippingservice", span_kind="SPAN_KIND_SERVER",status_code="STATUS_CODE_UNSET"} 1
      - name: http.status_code
```

* Under the `service: pipelines` list:

```yaml
  metrics/spanmetrics:
      # This receiver is just a dummy and never used.
      # Added to pass validation requiring at least one receiver in a pipeline.
      receivers: [otlp/spanmetrics]
      exporters: [prometheus]
```

Replace `<<TRACING-SHIPPING-TOKEN>>` with the token of the account you want to ship to.

If your account is hosted in any region other than `us`, replace `LOGZIO_ACCOUNT_REGION_CODE` with the applicable region code.

##### Start the collector

Run the following command:

`<path/to>/otelcontribcol_<VERSION-NAME> --config ./config.yaml`

* Replace `<path/to>` with the path to the directory where you downloaded the collector.
* Replace `<VERSION-NAME>` with the version name of the collector applicable to your system, e.g. `otelcontribcol_darwin_amd64`.

##### Run the application

Run the application to generate traces.

##### Check Logz.io for your metrics

Give your metrics some time to get from your system to ours, and then open Tracing. Navigate to the Monitor tab to view the span metrics.



<!--
#### To generate a Service Performance Monitoring dashboard: 
{:.no_toc}  



##### Create a Metrics account

_You must have admin permissions for the Logz.io account to view and edit the **Manage accounts** page._

1. Navigate to **Tracing** > <a href="https://app.logz.io/#/dashboard/settings/manage-accounts" target ="_blank"> **Manage accounts** > **Get my Metrics account**.
3. Choose a name for your account. The new Metrics account name **must contain** your Tracing account name.
4. Pick your preferred number of unique metrics and **Create** the account.
3. Copy the new Metrics account token.

![Account management screen](https://dytvr9ot2sszz.cloudfront.net/logz-docs/distributed-tracing/manage-accounts-metrics-creation.png)


##### Install and run the Span Metrics Processor

Logz.io captures end-to-end distributed transactions from your applications and infrastructure with trace spans sent via the Spanmetrics OpenTelemetry collector, which you install inside your environment.

The Span Metrics Processor processes spans on the go, emitting R.E.D metrics at configurable time intervals.

[This link takes you to the OpenTelemetry installation.](https://app.logz.io/#/dashboard/send-your-data/tracing-sources/opentelemetry) 
The information is also available in the **Logz.io Docs**, in [**Ship your data > OpenTelemetry installation**](https://docs.logz.io/shipping/tracing-sources/opentelemetry.html).

To configure your spanmetrics processor, make sure you use your **Metrics** account token, name and region.


```yaml
receivers:
  jaeger:
    protocols:
      grpc:
      thrift_binary:
      thrift_compact:
      thrift_http:
​
  otlp/spanmetrics:
    protocols:
      grpc:
        endpoint: :12345
  otlp:
    protocols:
      grpc:
        endpoint: :4317
  prometheus:
    config:
      global:
        external_labels:
          p8s_logzio_name: # Enter chosen name
      scrape_configs: 
      - job_name: '' # Enter job name
        scrape_interval: 15s
        static_configs:
        - targets: [ "0.0.0.0:8889" ]
​
exporters:
  logzio:
    account_token: " " # Enter your account token
    region: "us" # Enter your region
  prometheusremotewrite:
    endpoint: https://listener-us.logz.io:8053
    headers:
      Authorization: Bearer duwFfWzoOBTUgavSxtAacNWwXbwgPcnx
  prometheus:
    endpoint: "localhost:8889"
  logging:
​
processors:
  batch:
  spanmetrics:
    metrics_exporter: prometheus
    latency_histogram_buckets: [2ms, 6ms, 10ms, 100ms, 250ms, 500ms, 1000ms, 10000ms, 100000ms, 1000000ms]
    # Additional list of dimensions on top of:
    # - service.name
    # - operation
    # - span.kind
    # - status.code
    dimensions:
      # If the span is missing http.method, the processor will insert
      # the http.method dimension with value 'GET'.
      # For example, in the following scenario, http.method is not present in a span and so will be added as a dimension to the metric with value "GET":
      # - promexample_calls{http_method="GET",operation="/Address",service_name="shippingservice", span_kind="SPAN_KIND_SERVER",status_code="STATUS_CODE_UNSET"} 1
      - name: http.method
        default: GET
      # If a default is not provided, the http.status_code dimension will be omitted
      # if the span does not contain http.status_code.
      # For example, consider a scenario with two spans, one span having http.status_code=200 and another missing http.status_code. Two metrics would result with this configuration, one with the http_status_code omitted and the other included:
      # - promexample_calls{http_status_code="200",operation="/Address",service_name="shippingservice", span_kind="SPAN_KIND_SERVER",status_code="STATUS_CODE_UNSET"} 1
      # - promexample_calls{operation="/Address",service_name="shippingservice",span_kind="SPAN_KIND_SERVER", status_code="STATUS_CODE_UNSET"} 1
      - name: http.status_code    
​
extensions:
  pprof:
    endpoint: :1777
  zpages:
    endpoint: :55679
  health_check:
​
service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    traces:
      receivers: [jaeger]
      processors: [spanmetrics,batch]
      exporters: [logzio]
    metrics/spanmetrics:
      # This receiver is just a dummy and never used.
      # Added to pass validation requiring at least one receiver in a pipeline.
      receivers: [otlp/spanmetrics]
      exporters: [prometheus]
    metrics:
      receivers: [otlp,prometheus]
      exporters: [logging,prometheusremotewrite]      
  telemetry:
    logs:
      level: "error"
````

<!--
The OpenTelemetry repository offers configuration examples, such as:

* [Exporter not found](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/spanmetricsprocessor/testdata/config-exporter-not-found.yaml) - Where the configured 'metrics_exporter' within spanprocessor, cannot be found in any pipeline, leading to a config validation error
* [3-pipeline configuration](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/spanmetricsprocessor/testdata/config-3-pipelines.yaml) - When a user wishes to perform further processing of aggregated span metrics prior to exporting (traces -> metrics-proxy-pipeline -> metrics)
* [And more](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/spanmetricsprocessor/testdata)
-->
<!--
##### View your Service Performance Monitoring dashboard

Once you've finished the setup, the new dashboard will be available under your Tracing account. To view it, navigate to the [Tracing account](https://app.logz.io/#/dashboard/jaeger/monitoring), and click on the **Monitoring** tab:

![Service Performance Monitoring dashboard](https://dytvr9ot2sszz.cloudfront.net/logz-docs/distributed-tracing/spm-main-dashboard.png)

The dashboard includes a breakdown of R.E.D data (Requests, Errors and Delay) based on the operations running inside the chosen service.

To learn more about the Service Performance Monitoring dashboard, what it includes and how you can utilize it, **[check out our detailed overview](https://docs.logz.io/user-guide/distributed-tracing/service-performance-monitoring#service-performance-monitoring-dashboard)**.
-->
</div>